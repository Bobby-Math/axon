# Axon

**High-performance ML inference server in Rust**

Axon is a production-ready ML inference server optimized for GPU workloads, featuring dynamic batching, model caching, and sub-10ms p99 latency (target).

## Features

- âœ… Async Rust (Tokio) architecture
- âœ… GPU-accelerated inference via [Synapse](https://github.com/yourname/synapse)
- âœ… Dynamic batching for improved throughput (planned)
- âœ… Intelligent model caching (planned)
- âœ… Prometheus metrics and observability (planned)

## Use Cases

- **Embedded/edge deployment** - Lightweight, Rust-native
- **Safety-critical systems** - Memory safety guarantees
- **Rust-native stacks** - Zero Python overhead
- **Custom CUDA integration** - Direct control over GPU operations

## Quick Start

```rust
use axon::server::InferenceServer;

#[tokio::main]
async fn main() -> Result<(), String> {
    let server = InferenceServer::load("model.onnx").await?;
    let result = server.infer(&input_tensor).await?;
    Ok(())
}
```

## Architecture

```
Client Requests
    â†“
Axon Server (batching, caching, routing)
    â†“
Model Execution (ONNX Runtime - planned)
    â†“
Synapse (GPU operations)
    â†“
CUDA Kernels
```

## Not Competing with vLLM

Axon is designed for specialized use cases where Python-based frameworks don't fit:

- Embedded/edge deployments (lightweight)
- Safety-critical applications (memory safety)
- Rust-native infrastructure (no Python runtime)
- Custom CUDA kernel integration

**For mainstream LLM serving in Python environments, use [vLLM](https://github.com/vllm-project/vllm).**

## Status

ðŸš§ **Early Development** - Core architecture in place. ONNX Runtime integration in progress.

## Used By

- [Tessera](https://github.com/yourname/tessera) - Distributed GPU orchestration (private)

## License

Business Source License 1.1 (BSL-1.1)

Axon is licensed under the Business Source License 1.1. The license allows free use for non-production purposes and production use that doesn't compete with commercial ML inference offerings.

On **2029-11-27** (four years from initial publication), the license automatically converts to the MIT License, making Axon fully open source.

See the [LICENSE](LICENSE) file for complete terms.
